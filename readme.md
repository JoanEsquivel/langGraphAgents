# ü§ñ LangGraph QA Automation Agent with RAGAS Evaluation

> **Progressive AI Agent Development: From Basic Chat to Production-Ready QA Automation**

A comprehensive learning project that demonstrates the evolution of AI agents from basic chatbots to sophisticated QA automation consultants, complete with industry-standard RAGAS evaluation framework for measuring agent performance in real-world scenarios.

## üéØ Project Overview

This project showcases **progressive agent development** through four distinct stages:

1. **Basic Stateful Chatbot** (`1_basic_chat_bot.py`) - Foundation with conversation memory
2. **Tool-Enhanced Agent** (`2_basic_chat_bot_with_tools.py`) - Web search integration via Tavily
3. **Production Memory System** (`3_basic_chat_bot_with_tools_memory.py`) - Persistent conversation storage
4. **QA Automation Expert** (`4-final-agent-formated-response.py`) - Specialized consultant with RAGAS evaluation

Each stage builds upon the previous, demonstrating real-world AI development practices from prototype to production.

### üèÜ Key Features

- **Specialized QA Agent**: Expert knowledge in test automation, CI/CD, and software testing
- **Real Web Search**: Live integration with Tavily API for current information
- **Memory Persistence**: Conversation continuity across sessions
- **RAGAS Evaluation**: Industry-standard metrics for agent performance assessment
- **Local LLM Support**: Uses Ollama with Qwen2.5 model for privacy and control

## ‚ö° Quick Start

**For immediate testing:**

```bash
# 1. Ensure Ollama is running with Qwen2.5 model
ollama pull qwen2.5:7b-instruct && ollama serve

# 2. Set up environment  
python -m venv venv_foundational_agents
source venv_foundational_agents/bin/activate  # Windows: venv_foundational_agents\Scripts\activate
pip install -r requirements.txt

# 3. Configure API key
cp env.example .env
# Edit .env to add your Tavily API key

# 4. Test the QA automation agent
python src/4-final-agent-formated-response.py

# 5. Run RAGAS evaluation tests
python -m pytest tests/test_real_agent_simple.py -v
```

---

## üõ†Ô∏è Detailed Setup & Installation

### üìã Prerequisites

- **Python 3.9+** 
- **Ollama** (local LLM server)
- **Tavily API Key** (web search - required)
- **LangSmith API Key** (monitoring - optional)

### üöÄ Installation Steps

#### 1. Environment Setup
```bash
# Clone and enter directory
git clone <your-repo-url>
cd langGraphAgents

# Create virtual environment  
python -m venv venv_foundational_agents
source venv_foundational_agents/bin/activate  # Windows: venv_foundational_agents\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### 2. Ollama Setup
```bash
# Install Ollama (choose your platform)
brew install ollama                              # macOS
curl -fsSL https://ollama.ai/install.sh | sh     # Linux
# Windows: Download from https://ollama.ai/download

# Pull required model and start server
ollama pull qwen2.5:7b-instruct
ollama serve  # Runs on http://localhost:11434
```

#### 3. API Configuration
```bash
# Create environment file
cp env.example .env

# Edit .env with your API keys:
# TAVILY_API_KEY=your_tavily_key_here          # Required for web search
# LANGSMITH_API_KEY=your_langsmith_key_here    # Optional for monitoring
```

**Get API Keys:**
- **Tavily**: [tavily.com](https://tavily.com/) ‚Üí Dashboard ‚Üí API Keys
- **LangSmith**: [smith.langchain.com](https://smith.langchain.com/) ‚Üí Settings ‚Üí API Keys

#### 4. Test Installation
```bash
# Test the QA automation agent
python src/4-final-agent-formated-response.py

# Run RAGAS evaluation suite
python -m pytest tests/test_real_agent_simple.py -v
```

### üîß Quick Troubleshooting

| Issue | Solution |
|-------|----------|
| **Ollama connection failed** | `ollama serve` and ensure `ollama list` shows `qwen2.5:7b-instruct` |
| **Import errors** | Activate virtual environment: `source venv_foundational_agents/bin/activate` |
| **API key errors** | Check `.env` file format and verify keys are valid |
| **Test failures** | Run with verbose output: `pytest -v -s --tb=long` |

### ‚úÖ Verification Checklist

- [ ] Ollama running (`curl http://localhost:11434/api/version`)
- [ ] Model available (`ollama list | grep qwen2.5`)  
- [ ] Dependencies installed (`pip list | grep langgraph`)
- [ ] `.env` configured with Tavily API key
- [ ] Agent working (`python src/4-final-agent-formated-response.py`)
- [ ] Tests passing (`pytest tests/test_real_agent_simple.py`)

---

## üèóÔ∏è System Architecture

### üìä Component Architecture

```mermaid
graph TD
    subgraph "Agent Development Pipeline"
        A[Basic Chatbot<br/>State Management] --> B[Tool Integration<br/>Web Search]
        B --> C[Memory System<br/>Persistence]
        C --> D[Production Agent<br/>RAGAS Ready]
    end
    
    subgraph "RAGAS Evaluation System"
        D --> E[Conversation Capture]
        E --> F[Format Conversion]
        F --> G[Metric Evaluation]
        
        subgraph "Conversation Formatters"
            H[get_conversation_for_ragas<br/>LangChain Messages]
            I[get_conversation_for_tool_accuracy<br/>RAGAS Messages]
            J[get_conversation_for_goal_accuracy<br/>MultiTurnSample]
        end
        
        F --> H
        F --> I
        F --> J
    end
    
    subgraph "RAGAS Metrics"
        K[TopicAdherenceScore<br/>Topic Focus Analysis]
        L[ToolCallAccuracy<br/>Tool Usage Evaluation]
        M[AgentGoalAccuracyWithReference<br/>Task Completion Assessment]
    end
    
    H --> K
    I --> L
    J --> M
    
    style D fill:#e1f5fe
    style K fill:#f3e5f5
    style L fill:#e8f5e8
    style M fill:#fff3e0
```

### üèóÔ∏è Project Structure

```
langGraphAgents/
‚îú‚îÄ‚îÄ src/                                       # üìÅ Source Code
‚îÇ   ‚îú‚îÄ‚îÄ 1_basic_chat_bot.py                   # üü¢ Stage 1: Basic stateful conversation
‚îÇ   ‚îú‚îÄ‚îÄ 2_basic_chat_bot_with_tools.py        # üü° Stage 2: Tool integration (web search)
‚îÇ   ‚îú‚îÄ‚îÄ 3_basic_chat_bot_with_tools_memory.py # üü† Stage 3: Memory persistence system
‚îÇ   ‚îú‚îÄ‚îÄ 4-final-agent-formated-response.py    # üî¥ Stage 4: QA expert + RAGAS evaluation
‚îÇ   ‚îî‚îÄ‚îÄ utils/                                 # üõ†Ô∏è Utility modules
‚îÇ       ‚îú‚îÄ‚îÄ langchain_setup.py                # LangSmith configuration
‚îÇ       ‚îî‚îÄ‚îÄ tavily_setup.py                   # Tavily API integration
‚îÇ
‚îú‚îÄ‚îÄ tests/                                     # üß™ RAGAS Evaluation Suite
‚îÇ   ‚îú‚îÄ‚îÄ test_real_agent_simple.py            # Main evaluation tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                          # Test configuration & fixtures
‚îÇ   ‚îú‚îÄ‚îÄ helpers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                         # Test utilities & agent runner
‚îÇ   ‚îî‚îÄ‚îÄ logs/                                # Test execution logs
‚îÇ       ‚îú‚îÄ‚îÄ test_goal_accuracy_*.txt         # Goal achievement results
‚îÇ       ‚îú‚îÄ‚îÄ test_tool_accuracy_*.txt         # Tool usage results
‚îÇ       ‚îî‚îÄ‚îÄ test_topic_adherence_*.txt       # Topic focus results
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                          # üì¶ Python dependencies
‚îú‚îÄ‚îÄ env.example                              # üîê Environment configuration template
‚îî‚îÄ‚îÄ venv_foundational_agents/               # üêç Virtual environment (auto-created)
```

**Progressive Learning Path:**
1. **Green** ‚Üí Basic conversation with memory
2. **Yellow** ‚Üí Add web search capabilities  
3. **Orange** ‚Üí Persistent conversation storage
4. **Red** ‚Üí Specialized QA expert with evaluation

---

## üìä RAGAS Evaluation Framework

### üéØ Unified Evaluation Method

The project uses a **single unified method** for all RAGAS evaluation metrics, simplifying the testing process while maintaining comprehensive coverage:

```python
# Universal method for all RAGAS metrics
sample = getMultiTurnSampleConversation(thread_id)

# Configure for specific metrics:
sample.reference_topics = ["QA", "testing", "automation"]     # Topic Adherence
sample.reference_tool_calls = extracted_tool_calls           # Tool Accuracy  
sample.reference = "Expected goal achievement description"    # Goal Achievement
```

### üß™ Evaluation Metrics

| Metric | Purpose | What It Measures | Threshold |
|--------|---------|------------------|-----------|
| **Topic Adherence** | Focus verification | Agent stays within QA automation domain | ‚â• 0.8 |
| **Tool Call Accuracy** | Tool usage evaluation | Correct web search usage when needed | ‚â• 0.7 |
| **Goal Achievement** | Task completion assessment | Agent provides useful, complete responses | ‚â• 0.5 |

### üöÄ Real Test Performance

**Current test results with actual conversation data:**

```bash
‚úÖ Topic Adherence:  1.000/1.0 (PERFECT!)
‚úÖ Tool Accuracy:    1.000/1.0 (PERFECT!)  
‚úÖ Goal Achievement: 1.000/1.0 (PERFECT!)
```

### üìù Test Examples

#### Basic Topic Adherence Test
```python
@pytest.mark.asyncio
async def test_topic_adherence_simple():
    thread_id = f"topic_test_{uuid.uuid4().hex[:8]}"
    
    # Create conversation with off-topic challenge
    stream_graph_updates("What's the weather in Barcelona?", thread_id)
    stream_graph_updates("What are CI/CD best practices?", thread_id)
    
    # Evaluate topic focus
    sample = getMultiTurnSampleConversation(thread_id)
    sample.reference_topics = ["weather", "testing", "CI/CD", "automation"]
    
    scorer = TopicAdherenceScore(llm=evaluator_llm, mode="recall")
    score = await scorer.multi_turn_ascore(sample)
    
    assert score >= 0.4  # Agent should maintain professional focus
```

#### Tool Usage Evaluation
```python
@pytest.mark.asyncio  
async def test_tool_accuracy_simple():
    thread_id = f"tool_test_{uuid.uuid4().hex[:8]}"
    
    # Trigger web search requirement
    stream_graph_updates("Search for recent automation testing news", thread_id)
    
    # Evaluate tool usage
    sample = getMultiTurnSampleConversation(thread_id)
    # Extract and set reference tool calls from conversation
    sample.reference_tool_calls = extracted_tool_calls
    
    scorer = ToolCallAccuracy()
    score = await scorer.multi_turn_ascore(sample)
    
    assert score >= 0.7  # Agent should use tools appropriately
```

### üé≠ Score Interpretation

| Score Range | Status | Action Required |
|-------------|--------|----------------|
| **0.90 - 1.00** | üü¢ **Excellent** | Production ready - monitor for consistency |
| **0.75 - 0.89** | üü° **Good** | Minor tuning needed - review edge cases |
| **0.60 - 0.74** | üü† **Moderate** | Requires attention - adjust prompts/training |
| **< 0.60** | üî¥ **Needs Work** | Major revision required - review architecture |

---

## üîç Understanding RAGAS Metrics

### Metric Definitions & Expected Scoring Behavior

#### üéØ Topic Adherence Score
- **Purpose**: Measures how well the agent stays focused on reference topics
- **Method**: Multi-step LLM evaluation with precision/recall calculations
- **Architecture**: 
  1. Extracts topics from conversation using LLM
  2. Classifies if agent "answered" or "refused" each topic 
  3. Matches extracted topics against reference topics
  4. Calculates precision, recall, or F1 based on mode setting
- **Range**: 0.0 to 1.0 (supports **nuanced scoring**)
- **Expected Scores**:
  - **0.6-0.8**: Mixed topic handling (some on-topic, some boundaries crossed)
  - **0.8-0.9**: Good topic adherence with minor boundary issues
  - **0.9-1.0**: Excellent topic focus and boundary management
- **Configuration**: `mode="precision"` vs `mode="recall"` affects scoring sensitivity

#### üõ†Ô∏è Tool Call Accuracy  
- **Purpose**: Evaluates exact matching of tool calls against expected references
- **Method**: **Exact string matching** with sequence alignment
- **Architecture**:
  1. Compares tool names (must match exactly)
  2. Uses `ExactMatch()` metric for tool arguments (no fuzzy matching)
  3. Multiplies by sequence alignment score (binary: 0 or 1)
- **Range**: 0.0 to 1.0 (**binary behavior** in practice)
- **Expected Scores**:
  - **0.000**: Tool calls don't exactly match reference (common)
  - **1.000**: Perfect exact match of tool names and arguments
- **‚ö†Ô∏è Limitation**: No semantic understanding - "2024" ‚â† "2025" = complete failure

#### üèÜ Goal Achievement
- **Purpose**: Binary comparison of goal completion against reference
- **Method**: **Forced binary LLM evaluation** 
- **Architecture**:
  1. LLM infers goal and end state from conversation
  2. LLM compares desired outcome vs achieved outcome  
  3. Returns only "0" (different) or "1" (same) - no middle ground
- **Range**: **0.000 or 1.000 only** (explicitly binary)
- **Expected Scores**:
  - **0.000**: Goal not met OR missing any reference requirement (e.g., pets)
  - **1.000**: Goal perfectly achieved as specified in reference
- **‚ö†Ô∏è Design**: `output_type = MetricOutputType.BINARY` enforces 0/1 only

### üìä Real Test Performance Data

**Recent test results demonstrate the actual scoring behavior:**

#### Test 1: Topic Adherence (Mixed Reference Topics)
```
User: "What is functional testing?" ‚Üí "What's the weather today?"

Agent Actions:
1. Provides comprehensive functional testing explanation ‚úÖ
2. Redirects weather question back to QA domain ‚úÖ

Reference Topics: ['functional testing', 'software testing', 'test automation', 
                  'quality assurance', 'testing strategies', 'testing practices', 
                  'lifestyle', 'weather', 'cooking', 'general knowledge']

RAGAS Score: 0.600/1.0 (precision mode)
- Shows nuanced scoring based on mixed topic handling
```

#### Test 2: Tool Call Accuracy (Exact Match Required)  
```
User: "Find current information about newest API testing framework released in 2024"

Agent: tavily_search(query="newest API testing framework 2024", search_depth="advanced")
Expected: tavily_search(query="newest API testing framework 2025")

RAGAS Score: 0.000/1.0 
- Binary failure due to "2024" ‚â† "2025" exact match requirement
```

#### Test 3: Goal Achievement (Binary Evaluation)
```
User: "What is API testing and why is it important?"

Agent: Comprehensive explanation with types, best practices, tools, examples
Reference: "Explained what API testing is, described its importance..."

RAGAS Score: 1.000/1.0
- Binary success when all reference requirements met exactly
```

### üéÆ Running Your Own Evaluations

```python
# Basic evaluation workflow
thread_id = f"test_{uuid.uuid4().hex[:8]}"

# 1. Generate conversation
stream_graph_updates("Your test question here", thread_id)

# 2. Convert to RAGAS format  
sample = getMultiTurnSampleConversation(thread_id)

# 3. Configure for your metric
sample.reference_topics = ["your", "topics", "here"]  # Topic adherence
# OR sample.reference = "Your expected outcome"        # Goal achievement  
# OR sample.reference_tool_calls = tool_calls          # Tool accuracy

# 4. Evaluate
scorer = YourChosenMetric(llm=evaluator_llm)
score = await scorer.multi_turn_ascore(sample)
```

### ‚öôÔ∏è Scoring Configuration & Tuning

#### Topic Adherence Modes
```python
# More lenient scoring (higher scores)
scorer = TopicAdherenceScore(llm=llm, mode="recall")

# Stricter scoring (lower scores)  
scorer = TopicAdherenceScore(llm=llm, mode="precision")

# Balanced scoring
scorer = TopicAdherenceScore(llm=llm, mode="f1")  # Default
```

#### Tool Call Accuracy Tuning
```python
# Current implementation uses exact matching:
# "2024" ‚â† "2025" ‚Üí Score: 0.000

# For better results, ensure reference tool calls match expected agent behavior:
expected_tool_calls = [
    ToolCall(name="tavily_search", args={"query": "newest API testing framework 2024"})
    # ‚Üë Use agent's actual year, not a different one
]
```

#### Goal Achievement Reference Design
```python
# ‚úÖ Good: Achievable, clear requirements
reference_goal = "Explained API testing definition and importance"

# ‚ùå Bad: Impossible or mixed requirements  
reference_goal = "Explained API testing and mentioned pets"  # ‚Üí Always 0.000
```

### üéØ Expected Score Ranges in Practice

| Metric | Low (0.0-0.3) | Moderate (0.4-0.6) | Good (0.7-0.8) | Excellent (0.9-1.0) |
|--------|---------------|---------------------|-----------------|---------------------|
| **Topic Adherence** | Major off-topic drift | Mixed boundaries | Good focus + minor drift | Perfect domain adherence |
| **Tool Accuracy** | Wrong tools/args | N/A (binary) | N/A (binary) | Exact match only |  
| **Goal Achievement** | Missing requirements | N/A (binary) | N/A (binary) | Perfect match only |

**üîß Practical Tips:**
- **Topic Adherence**: Use mixed reference topics to test boundary handling (like lifestyle + QA topics)
- **Tool Accuracy**: Design reference calls to match agent's actual logic, not ideal behavior
- **Goal Achievement**: Keep references achievable - avoid impossible requirements

---

## üéì Learning Path & Dependencies  

### üìö Progressive Stages Explained

| Stage | File | Key Concepts | Dependencies |
|-------|------|-------------|-------------|
| **üü¢ Stage 1** | `1_basic_chat_bot.py` | LangGraph basics, State management, Memory | `langgraph`, `langchain-ollama` |
| **üü° Stage 2** | `2_basic_chat_bot_with_tools.py` | Tool integration, Web search, API calls | + `langchain-tavily` |
| **üü† Stage 3** | `3_basic_chat_bot_with_tools_memory.py` | Persistent memory, Conversation continuity | + `InMemorySaver` checkpointing |
| **üî¥ Stage 4** | `4-final-agent-formated-response.py` | Domain expertise, RAGAS evaluation | + `ragas`, specialized prompts |

### üß∞ Complete Dependencies

**Core Requirements:**
```txt
langgraph==0.6.5          # Agent framework & state management
langchain-ollama           # Local LLM integration (Qwen2.5)
langchain-tavily           # Web search capabilities  
langsmith==0.4.14         # Optional: Monitoring & tracing
python-dotenv==1.0.1      # Environment configuration
ragas                     # AI evaluation framework
pytest                    # Testing infrastructure
pytest-asyncio            # Async test support
requests                  # HTTP utilities
```

### üåü What You'll Learn

1. **üü¢ Foundation**: Build stateful conversations with local LLMs
2. **üü° Integration**: Connect agents to external APIs and tools
3. **üü† Production**: Implement memory persistence and conversation management
4. **üî¥ Specialization**: Create domain-specific agents with professional evaluation

### üöÄ Next Steps

- **Modify Prompts**: Adapt the QA focus to your domain (DevOps, Security, etc.)
- **Add Tools**: Integrate additional APIs beyond web search
- **Extend Evaluation**: Create custom RAGAS metrics for your specific use case
- **Scale Architecture**: Deploy with production databases and persistent storage

---

## ü§ù Contributing & Support

### üìÑ License
This project is provided as educational material for learning LangGraph and RAGAS evaluation.

### üîó Resources
- **LangGraph Documentation**: [langchain-ai.github.io/langgraph](https://langchain-ai.github.io/langgraph/)
- **RAGAS Framework**: [ragas.io](https://ragas.io/)
- **Ollama Models**: [ollama.ai/library](https://ollama.ai/library)
- **Tavily Search API**: [tavily.com](https://tavily.com/)

---

**üéØ Ready to build your own AI agent? Start with Stage 1 and work your way up to production-ready evaluation!**