# ü§ñ LangGraph QA Automation Agent with RAGAS Evaluation

> **Progressive AI Agent Development: From Basic Chat to Production-Ready QA Automation**

A comprehensive learning project that demonstrates the evolution of AI agents from basic chatbots to sophisticated QA automation consultants, complete with industry-standard RAGAS evaluation framework for measuring agent performance in real-world scenarios.

## üéØ Project Overview

This project showcases **progressive agent development** through four distinct stages:

1. **Basic Stateful Chatbot** (`1_basic_chat_bot.py`) - Foundation with conversation memory
2. **Tool-Enhanced Agent** (`2_basic_chat_bot_with_tools.py`) - Web search integration via Tavily
3. **Production Memory System** (`3_basic_chat_bot_with_tools_memory.py`) - Persistent conversation storage
4. **QA Automation Expert** (`4-final-agent-formated-response.py`) - Specialized consultant with RAGAS evaluation

Each stage builds upon the previous, demonstrating real-world AI development practices from prototype to production.

### üèÜ Key Features

- **Specialized QA Agent**: Expert knowledge in test automation, CI/CD, and software testing
- **Real Web Search**: Live integration with Tavily API for current information
- **Memory Persistence**: Conversation continuity across sessions
- **RAGAS Evaluation**: Industry-standard metrics for agent performance assessment
- **Local LLM Support**: Uses Ollama with Qwen2.5 model for privacy and control

## ‚ö° Quick Start

**For immediate testing:**

```bash
# 1. Ensure Ollama is running with Qwen2.5 model
ollama pull qwen2.5:7b-instruct && ollama serve

# 2. Set up environment  
python -m venv venv_foundational_agents
source venv_foundational_agents/bin/activate  # Windows: venv_foundational_agents\Scripts\activate
pip install -r requirements.txt

# 3. Configure API key
cp env.example .env
# Edit .env to add your Tavily API key

# 4. Test the QA automation agent
python src/4-final-agent-formated-response.py

# 5. Run RAGAS evaluation tests
python -m pytest tests/test_real_agent_simple.py -v
```

---

## üõ†Ô∏è Detailed Setup & Installation

### üìã Prerequisites

- **Python 3.9+** 
- **Ollama** (local LLM server)
- **Tavily API Key** (web search - required)
- **LangSmith API Key** (monitoring - optional)

### üöÄ Installation Steps

#### 1. Environment Setup
```bash
# Clone and enter directory
git clone <your-repo-url>
cd langGraphAgents

# Create virtual environment  
python -m venv venv_foundational_agents
source venv_foundational_agents/bin/activate  # Windows: venv_foundational_agents\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### 2. Ollama Setup
```bash
# Install Ollama (choose your platform)
brew install ollama                              # macOS
curl -fsSL https://ollama.ai/install.sh | sh     # Linux
# Windows: Download from https://ollama.ai/download

# Pull required model and start server
ollama pull qwen2.5:7b-instruct
ollama serve  # Runs on http://localhost:11434
```

#### 3. API Configuration
```bash
# Create environment file
cp env.example .env

# Edit .env with your API keys:
# TAVILY_API_KEY=your_tavily_key_here          # Required for web search
# LANGSMITH_API_KEY=your_langsmith_key_here    # Optional for monitoring
```

**Get API Keys:**
- **Tavily**: [tavily.com](https://tavily.com/) ‚Üí Dashboard ‚Üí API Keys
- **LangSmith**: [smith.langchain.com](https://smith.langchain.com/) ‚Üí Settings ‚Üí API Keys

#### 4. Test Installation
```bash
# Test the QA automation agent
python src/4-final-agent-formated-response.py

# Run RAGAS evaluation suite
python -m pytest tests/test_real_agent_simple.py -v
```

### üîß Quick Troubleshooting

| Issue | Solution |
|-------|----------|
| **Ollama connection failed** | `ollama serve` and ensure `ollama list` shows `qwen2.5:7b-instruct` |
| **Import errors** | Activate virtual environment: `source venv_foundational_agents/bin/activate` |
| **API key errors** | Check `.env` file format and verify keys are valid |
| **Test failures** | Run with verbose output: `pytest -v -s --tb=long` |

### ‚úÖ Verification Checklist

- [ ] Ollama running (`curl http://localhost:11434/api/version`)
- [ ] Model available (`ollama list | grep qwen2.5`)  
- [ ] Dependencies installed (`pip list | grep langgraph`)
- [ ] `.env` configured with Tavily API key
- [ ] Agent working (`python src/4-final-agent-formated-response.py`)
- [ ] Tests passing (`pytest tests/test_real_agent_simple.py`)

---

## üèóÔ∏è System Architecture

### üìä Component Architecture

```mermaid
graph TD
    subgraph "Agent Development Pipeline"
        A[Basic Chatbot<br/>State Management] --> B[Tool Integration<br/>Web Search]
        B --> C[Memory System<br/>Persistence]
        C --> D[Production Agent<br/>RAGAS Ready]
    end
    
    subgraph "RAGAS Evaluation System"
        D --> E[Conversation Capture]
        E --> F[Format Conversion]
        F --> G[Metric Evaluation]
        
        subgraph "Conversation Formatters"
            H[get_conversation_for_ragas<br/>LangChain Messages]
            I[get_conversation_for_tool_accuracy<br/>RAGAS Messages]
            J[get_conversation_for_goal_accuracy<br/>MultiTurnSample]
        end
        
        F --> H
        F --> I
        F --> J
    end
    
    subgraph "RAGAS Metrics"
        K[TopicAdherenceScore<br/>Topic Focus Analysis]
        L[ToolCallAccuracy<br/>Tool Usage Evaluation]
        M[AgentGoalAccuracyWithReference<br/>Task Completion Assessment]
    end
    
    H --> K
    I --> L
    J --> M
    
    style D fill:#e1f5fe
    style K fill:#f3e5f5
    style L fill:#e8f5e8
    style M fill:#fff3e0
```

### üèóÔ∏è Project Structure

```
langGraphAgents/
‚îú‚îÄ‚îÄ src/                                       # üìÅ Source Code
‚îÇ   ‚îú‚îÄ‚îÄ 1_basic_chat_bot.py                   # üü¢ Stage 1: Basic stateful conversation
‚îÇ   ‚îú‚îÄ‚îÄ 2_basic_chat_bot_with_tools.py        # üü° Stage 2: Tool integration (web search)
‚îÇ   ‚îú‚îÄ‚îÄ 3_basic_chat_bot_with_tools_memory.py # üü† Stage 3: Memory persistence system
‚îÇ   ‚îú‚îÄ‚îÄ 4-final-agent-formated-response.py    # üî¥ Stage 4: QA expert + RAGAS evaluation
‚îÇ   ‚îî‚îÄ‚îÄ utils/                                 # üõ†Ô∏è Utility modules
‚îÇ       ‚îú‚îÄ‚îÄ langchain_setup.py                # LangSmith configuration
‚îÇ       ‚îî‚îÄ‚îÄ tavily_setup.py                   # Tavily API integration
‚îÇ
‚îú‚îÄ‚îÄ tests/                                     # üß™ RAGAS Evaluation Suite
‚îÇ   ‚îú‚îÄ‚îÄ test_real_agent_simple.py            # Main evaluation tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                          # Test configuration & fixtures
‚îÇ   ‚îú‚îÄ‚îÄ helpers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                         # Test utilities & agent runner
‚îÇ   ‚îî‚îÄ‚îÄ logs/                                # Test execution logs
‚îÇ       ‚îú‚îÄ‚îÄ test_goal_accuracy_*.txt         # Goal achievement results
‚îÇ       ‚îú‚îÄ‚îÄ test_tool_accuracy_*.txt         # Tool usage results
‚îÇ       ‚îî‚îÄ‚îÄ test_topic_adherence_*.txt       # Topic focus results
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                          # üì¶ Python dependencies
‚îú‚îÄ‚îÄ env.example                              # üîê Environment configuration template
‚îî‚îÄ‚îÄ venv_foundational_agents/               # üêç Virtual environment (auto-created)
```

**Progressive Learning Path:**
1. **Green** ‚Üí Basic conversation with memory
2. **Yellow** ‚Üí Add web search capabilities  
3. **Orange** ‚Üí Persistent conversation storage
4. **Red** ‚Üí Specialized QA expert with evaluation

---

## üìä RAGAS Evaluation Framework

### üéØ Unified Evaluation Method

The project uses a **single unified method** for all RAGAS evaluation metrics, simplifying the testing process while maintaining comprehensive coverage:

```python
# Universal method for all RAGAS metrics
sample = getMultiTurnSampleConversation(thread_id)

# Configure for specific metrics:
sample.reference_topics = ["QA", "testing", "automation"]     # Topic Adherence
sample.reference_tool_calls = extracted_tool_calls           # Tool Accuracy  
sample.reference = "Expected goal achievement description"    # Goal Achievement
```

### üß™ Evaluation Metrics

| Metric | Purpose | What It Measures | Threshold |
|--------|---------|------------------|-----------|
| **Topic Adherence** | Focus verification | Agent stays within QA automation domain | ‚â• 0.8 |
| **Tool Call Accuracy** | Tool usage evaluation | Correct web search usage when needed | ‚â• 0.7 |
| **Goal Achievement** | Task completion assessment | Agent provides useful, complete responses | ‚â• 0.5 |

### üöÄ Real Test Performance

**Current test results with actual conversation data:**

```bash
‚úÖ Topic Adherence:  1.000/1.0 (PERFECT!)
‚úÖ Tool Accuracy:    1.000/1.0 (PERFECT!)  
‚úÖ Goal Achievement: 1.000/1.0 (PERFECT!)
```

### üìù Test Examples

#### Basic Topic Adherence Test
```python
@pytest.mark.asyncio
async def test_topic_adherence_simple():
    thread_id = f"topic_test_{uuid.uuid4().hex[:8]}"
    
    # Create conversation with off-topic challenge
    stream_graph_updates("What's the weather in Barcelona?", thread_id)
    stream_graph_updates("What are CI/CD best practices?", thread_id)
    
    # Evaluate topic focus
    sample = getMultiTurnSampleConversation(thread_id)
    sample.reference_topics = ["weather", "testing", "CI/CD", "automation"]
    
    scorer = TopicAdherenceScore(llm=evaluator_llm, mode="recall")
    score = await scorer.multi_turn_ascore(sample)
    
    assert score >= 0.4  # Agent should maintain professional focus
```

#### Tool Usage Evaluation
```python
@pytest.mark.asyncio  
async def test_tool_accuracy_simple():
    thread_id = f"tool_test_{uuid.uuid4().hex[:8]}"
    
    # Trigger web search requirement
    stream_graph_updates("Search for recent automation testing news", thread_id)
    
    # Evaluate tool usage
    sample = getMultiTurnSampleConversation(thread_id)
    # Extract and set reference tool calls from conversation
    sample.reference_tool_calls = extracted_tool_calls
    
    scorer = ToolCallAccuracy()
    score = await scorer.multi_turn_ascore(sample)
    
    assert score >= 0.7  # Agent should use tools appropriately
```

### üé≠ Score Interpretation

| Score Range | Status | Action Required |
|-------------|--------|----------------|
| **0.90 - 1.00** | üü¢ **Excellent** | Production ready - monitor for consistency |
| **0.75 - 0.89** | üü° **Good** | Minor tuning needed - review edge cases |
| **0.60 - 0.74** | üü† **Moderate** | Requires attention - adjust prompts/training |
| **< 0.60** | üî¥ **Needs Work** | Major revision required - review architecture |

---

## üîç Understanding RAGAS Metrics

### Metric Definitions

#### üéØ Topic Adherence Score
- **Purpose**: Measures how well the agent stays focused on QA automation topics
- **Method**: Compares responses against reference topics using LLM evaluation  
- **Range**: 0.0 (completely off-topic) to 1.0 (perfectly focused)

#### üõ†Ô∏è Tool Call Accuracy
- **Purpose**: Evaluates appropriate use of web search tools
- **Method**: Compares actual vs expected tool usage patterns
- **Range**: 0.0 (poor tool usage) to 1.0 (optimal tool selection)

#### üèÜ Goal Achievement
- **Purpose**: Assesses task completion quality against reference standards
- **Method**: LLM evaluation of response completeness and usefulness
- **Range**: 0.0 (goal not met) to 1.0 (perfect goal achievement)

### üìä Real Test Performance Data

**Sample conversation thread: `goal_test_c8af1eb3`**

```
User: "Research latest test automation frameworks and provide a summary"

Agent Actions:
1. Recognizes need for current information ‚Üí triggers web search
2. Calls tavily_search with query: "latest test automation frameworks"  
3. Processes real web results from TestGuild, Testomat.io, and other sources
4. Delivers structured summary of 7+ frameworks with descriptions

RAGAS Scores:
‚úÖ Topic Adherence:  1.000/1.0 (stayed within QA domain)
‚úÖ Tool Accuracy:    1.000/1.0 (used search appropriately)  
‚úÖ Goal Achievement: 1.000/1.0 (comprehensive framework summary)
```

### üéÆ Running Your Own Evaluations

```python
# Basic evaluation workflow
thread_id = f"test_{uuid.uuid4().hex[:8]}"

# 1. Generate conversation
stream_graph_updates("Your test question here", thread_id)

# 2. Convert to RAGAS format  
sample = getMultiTurnSampleConversation(thread_id)

# 3. Configure for your metric
sample.reference_topics = ["your", "topics", "here"]  # Topic adherence
# OR sample.reference = "Your expected outcome"        # Goal achievement  
# OR sample.reference_tool_calls = tool_calls          # Tool accuracy

# 4. Evaluate
scorer = YourChosenMetric(llm=evaluator_llm)
score = await scorer.multi_turn_ascore(sample)
```

---

## üéì Learning Path & Dependencies  

### üìö Progressive Stages Explained

| Stage | File | Key Concepts | Dependencies |
|-------|------|-------------|-------------|
| **üü¢ Stage 1** | `1_basic_chat_bot.py` | LangGraph basics, State management, Memory | `langgraph`, `langchain-ollama` |
| **üü° Stage 2** | `2_basic_chat_bot_with_tools.py` | Tool integration, Web search, API calls | + `langchain-tavily` |
| **üü† Stage 3** | `3_basic_chat_bot_with_tools_memory.py` | Persistent memory, Conversation continuity | + `InMemorySaver` checkpointing |
| **üî¥ Stage 4** | `4-final-agent-formated-response.py` | Domain expertise, RAGAS evaluation | + `ragas`, specialized prompts |

### üß∞ Complete Dependencies

**Core Requirements:**
```txt
langgraph==0.6.5          # Agent framework & state management
langchain-ollama           # Local LLM integration (Qwen2.5)
langchain-tavily           # Web search capabilities  
langsmith==0.4.14         # Optional: Monitoring & tracing
python-dotenv==1.0.1      # Environment configuration
ragas                     # AI evaluation framework
pytest                    # Testing infrastructure
pytest-asyncio            # Async test support
requests                  # HTTP utilities
```

### üåü What You'll Learn

1. **üü¢ Foundation**: Build stateful conversations with local LLMs
2. **üü° Integration**: Connect agents to external APIs and tools
3. **üü† Production**: Implement memory persistence and conversation management
4. **üî¥ Specialization**: Create domain-specific agents with professional evaluation

### üöÄ Next Steps

- **Modify Prompts**: Adapt the QA focus to your domain (DevOps, Security, etc.)
- **Add Tools**: Integrate additional APIs beyond web search
- **Extend Evaluation**: Create custom RAGAS metrics for your specific use case
- **Scale Architecture**: Deploy with production databases and persistent storage

---

## ü§ù Contributing & Support

### üìÑ License
This project is provided as educational material for learning LangGraph and RAGAS evaluation.

### üîó Resources
- **LangGraph Documentation**: [langchain-ai.github.io/langgraph](https://langchain-ai.github.io/langgraph/)
- **RAGAS Framework**: [ragas.io](https://ragas.io/)
- **Ollama Models**: [ollama.ai/library](https://ollama.ai/library)
- **Tavily Search API**: [tavily.com](https://tavily.com/)

---

**üéØ Ready to build your own AI agent? Start with Stage 1 and work your way up to production-ready evaluation!**